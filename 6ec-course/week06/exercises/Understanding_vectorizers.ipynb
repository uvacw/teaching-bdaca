{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4aa57f",
   "metadata": {},
   "source": [
    "# Understanding vectorizers\n",
    "\n",
    "In the following code examples, we will experiment with vectorizers to understand a bit better how they work. Feel free to adjust the code, and try things out yourself.\n",
    "\n",
    "For now, we will practice with `sklearn`'s vectorizers. however, packages such as `gensim` offer their own build in functionality to vectorize the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29eb18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4273fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"hello students!\", \"how are you today?\", \"what?\", \"hello hello everybody\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b662da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenization (manually), using .split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e59b046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'students!'],\n",
       " ['how', 'are', 'you', 'today?'],\n",
       " ['what?'],\n",
       " ['hello', 'hello', 'everybody']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts = [i.split() for i in texts]\n",
    "tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8149b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenization (manually), more advanced, using TreebankwordTokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "286e9e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'students', '!'],\n",
       " ['how', 'are', 'you', 'today', '?'],\n",
       " ['what', '?'],\n",
       " ['hello', 'hello', 'everybody']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenized_texts  = [TreebankWordTokenizer().tokenize(i) for i in texts]\n",
    "tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sklearn will vectorizer your data for you under the hood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4bf0333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.CountVectorizer"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 1))# initialize the vectorizer\n",
    "type(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4e71b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vect.fit_transform(texts)#fit the vectorizer and transform the documents in one go\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0372eb2",
   "metadata": {},
   "source": [
    "## Example 1: Inspect the output of a vectorizer in a dense format\n",
    "\n",
    "The following code cell will fit and transform three documents using a `Count`-based vectorizer. Next, the output is transformed to a *dense* matrix, and printed. \n",
    "\n",
    "1. Do you understand the output?\n",
    "2. Is it smart to transform output to a dense format? What will happen if you work with millions of documents, rather than 3 short sentences?\n",
    "3. what happens if you replace `CountVectorizer()` for `TfidfVectorizer()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fe0a32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  everybody  hello  how  students  today  what  you\n",
      "0    0          0      1    0         1      0     0    0\n",
      "1    1          0      0    1         0      1     0    1\n",
      "2    0          0      0    0         0      0     1    0\n",
      "3    0          1      2    0         0      0     0    0\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(X.A, columns=vect.get_feature_names()).to_string())\n",
    "df = pd.DataFrame(X.toarray().transpose(), index = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0c0c5",
   "metadata": {},
   "source": [
    "## Example 2: Inspect the output of a vectorizer in a sparse format\n",
    "\n",
    "Internally, `sklearn` represents the data in a *sparse* format, as this is computationally more efficient, and less memory is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ea857d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"hello students!\", \"how are you today?\", \"what?\", \"hello hello everybody\"]\n",
    "count_vec = CountVectorizer(ngram_range=(1,1)) #initilize the vectorizer\n",
    "count_vec_fit = count_vec.fit_transform(texts) #fit the vectorizer and transform the documents in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed890fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_fit = count_vec.transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03354120",
   "metadata": {},
   "source": [
    "    1.Inspect the shape of transformed texts. We can see that we have a 4x8 sparse matrix, meaning that we have 4 \n",
    "    rows (=documents) and 8 unique tokens (=words, numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe603bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd6f01",
   "metadata": {},
   "source": [
    "    2.Get the feature names. This will return the tokens that are in the vocabulary of the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a925f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are', 'everybody', 'hello', 'how', 'students', 'today', 'what', 'you']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2c6f3",
   "metadata": {},
   "source": [
    "    3. Represent the token's mapping to it's id values. The numbers do *not* represent the count of the words but the position of the words in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0937869d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 2,\n",
       " 'students': 4,\n",
       " 'how': 3,\n",
       " 'are': 0,\n",
       " 'you': 7,\n",
       " 'today': 5,\n",
       " 'what': 6,\n",
       " 'everybody': 1}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.vocabulary_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b768f3b",
   "metadata": {},
   "source": [
    "    4. Get sparse representation on document level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62d39871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello students!\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "\n",
      "how are you today?\n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "\n",
      "what?\n",
      "  (0, 6)\t1\n",
      "\n",
      "hello hello everybody\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, document in zip(count_vec_fit, texts):\n",
    "    print(document)\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4a49e",
   "metadata": {},
   "source": [
    "a. Do you understand the output printed above?  \n",
    "b. What happens if you change the `count` to a `tfidf` vectorizer?  \n",
    "c. Adjust the code using the slides of [this week](https://github.com/annekroon/CCS-2/blob/main/week02/week02-lecture.pdf), page 40. More specifically, try removing stopwords, pruning and see how your results are affected. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
